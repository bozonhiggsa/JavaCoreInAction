package net.core.tutorial.elementary._17_Charsets;

import java.io.UnsupportedEncodingException;
import java.nio.charset.Charset;
import java.nio.charset.StandardCharsets;
import java.util.Arrays;

/**

 Кодировка – это соответствие между символами и числами.

 ASCII – кодировка, которая включает в себя управляющие символы, знаки препинания, десятичные цифры,
 латинский алфавит. Коды символов лежат в диапазоне 0-127 (0-7F).

 Большинство распространённых кодировок включают в себя ASCII составной частью.

 cp1251 (Windows1251) – однобайтная кодировка (0-255 (0-FF)), содержащая помимо символов ASCII так же символы кирилицы.

 KOI8 – однобайтная кодировка, содержащая символы кирилицы. Есть подвиды: KOI8-R (cp20866) – содержит русский алфавит,
 KOI8-U (cp21866) – содержит украинский алфавит.

 Cp866 – однобайтная кодировка, содержащая символы кирилицы.

 UNICODE – унифицированная кодировка, в которой содержаться все существующие символы. Каждому символу UNICODE
 соответствует фиксированный числовой код в виде неотрицательного целого числа (codepoint).
 Кирилические символы имеют codepoint начиная с 1040.
 Первые 0-127 (0-7F) символов UNICODE – это символы ASCII.
 Первые 0-255 (0-FF) символов UNICODE – это символы кодировки Latin1 (ISO-8859-1).
 Диапазон UNICODE 0-FFFF – основные языки мира (BMP – базовая мультиязыковая плоскость).
 Диапазон UNICODE 10000-10FFFF – дополнительные символы. Эти символы представляются в Java в виде двух char.
 Не каждому codepoint в UNICODE соответствует символ, некоторым codepoint не поставлено в соответствие
 ни одного символа.

 UTF – Unicode transformation format – способ отображения codepoint UNICODE в последовательность байт.

 UTF-8 – кодировка переменной длины (1-6 байт). Символы ASCII занимают 1 байт, кирилические символы 2 байта,
 японские – 3 байта.

 UTF-16 имеет фиксированную длину  2 байта на 1 символ из BMP (0-FFFF) и 4 байта
 для дополнительных символов (10000-10FFFF).

 UTF-32 имеет фиксированную длину 4 байта.

 Вместе с кодировками UTF-16 и UTF-32 может использоваться метка порядка байт (BOM).
 UTF-16 BE – прямой порядок байт, FEFF – старший байт находится впереди младшего.
 UTF-16 LE – обратный порядок байт, FFEF – младший байт находится впереди старшего.
 UTF-32 BE – прямой порядок байт, 0000 FEFF
 UTF-32 LE – обратный порядок байт, FFEF 0000
 Если метка BOM в названии кодировки отсутствует, то по дефолту используется прямой порядок байт (BE).

 Символы в Java можно записать как:

 'd'
 '\uAB2F' // в виде 16-тиричного кода из UNICODE
 '\76' // в виде 8-миричного кода (допустимы также варианты '\X' и '\XXX')

 Можно использовать пробельные символы (управляющие символы ASCII). В частности:
 '\t' – горизонтальная табуляция (HT - codepoint 9)
 '\n' – перевод на новую строку (LF - codepoint 10)
 '\r' – возврат каретки (CR - codepoint 13)
 пробел (SP – codepoint 32)
 '\r\n' комбинация символов CRLF

 Использование символов '\n' и '\r' внутри строки, например в sout, не приводит к разрыву строки и поэтому допустимо.
 Например:
 Integer num = null;
 System.out.println("5" + num + '\n' + 10);
 даст результат:
 5null
 10

 Но использование аналогичных им символов '\u000a' и '\u000d' в Java запрещено!

 Если строка считана с неправильно указанной кодировкой, то после далеко не всегда её можно будет
 перекодировать без ошибок. Например, файл с символами кирилицы имеет кодировку UTF-8. В UTF-8 символ "И"
 кодируется последовательностью из двух байт: 0xD0, 0x98. При попытке считать строки в однобайтной кодировке
 cp1251 (windows-1251) обнаруживается, что код 0x98 не определен в наборе символов cp1251.

 Поэтому, если попытаться перекодировать строки в кодировку UTF-8 при помощи

 byte[] bytes = s.getBytes("cp1251");
 s = new String(bytes, Charset.forName("UTF-8"));

 то вместо символа "И" получим "?"

 Частые проблемы с кодировкой:
 1.	Прямоугольники вместо символов – в системе не установлен требуемый шрифт, который содержит данные символы;

 2.	"?" вместо символов – скорее всего причина в ошибке при декодировании входного потока
 (неверно указана кодировка) – декодер не в состоянии распознать поток байтов, они не соответствуют
 схеме кодирования, которую он ожидает;

 3.	Символы есть, но не те. Символов может быть существенно больше, чем в исходной строке – нелогичных
 вопросительных знаков нет – значит, декодер не нашел ошибок. Символы отображаются – значит, они есть
 в шрифте – скорее всего неверно выбрана кодировка и декодер пытается распознать двух-байтную кодировку
 как однобайтную (и др. варианты).

 */

public class _01_Charsets {

    public static void main(String[] args) {

        char ch1 = 0; // минимальное значение
        char ch2 = 65535; // максимальное значение
        char ch3 = 10000;
        char ch5 = 10_000;
        char ch6 = 0x0000; // минимум в 16-ричном коде
        char ch7 = 0xffff; // максимум в 16-ричном коде
        char ch8 = 0x0f1f; // 16-ричный код символа
        char ch9 = '\u0f1f'; // тот же 16-ричный код символа в Unicode
        String str = "\u0f1f\u0f1fA\u0f1f"; // 16-ричный код символа в Unicode удобно использовать для объединения
        // в строку известных символов и кодов неизвестных
        char ch10 = 'Y'; // сам символ
        int codepoint = (int) 'R'; // получение кода символа в алфавите Unicode, можно и без явного приведения типа

        for (char ch = 'a'; ch <= 'z'; ch++) { // так как символы это ещё и числа
            System.out.print(ch + " ");
        }
        System.out.println();

        Character ch11 = 'F'; // Две эти записи равноценны
        Character ch12 = new Character('F'); // В каждом классе обёртке есть поле value
        // с соответсвующим обёртываемым типом
        // но также в классе обёртке определены много библиотечных методов
        // и констант
        char[] chars = Character.toChars(1000); // возвращает символ по его codepoint - номеру в алфавите Unicode
        char[] chars2 = Character.toChars(150000); // так как в Unicode более 100000 символов, то для некоторых
        int lenchars = chars2.length; // требуется не 2 байта, а 4, т.е. 2 char
        // по одиночке эти два char не имеют визуализации
        System.out.println(Arrays.toString(chars) + " " +  chars.length + " " + Arrays.toString(chars2) + " " + lenchars);

        Charset utf16 = StandardCharsets.UTF_16;  // задание charset, т.е. правила преобразования
        // codepoint в массив байт
        byte[] bytes = "ABCDE".getBytes(utf16);  // преобразование строки (массива чаров) в массив байт
        System.out.println(Arrays.toString(bytes)); // UTF умеет преобразовывать в байты любой символ Unicode

        String s = new String(bytes, utf16); // при обратном преобразовании массива байтов в строку тоже надо
        // указывать кодировку
        // при необходимости перекодировать массив байт из одной кодировки
        // в другую, одну кодировку надо перевести в массив char (т.е. в строку)
        // а затем перегнать его в массив байтов с другой кодировкой

        System.out.println(s);

        System.out.println("----------------------------");

        String troubleString = "И вот пример строки";
        byte[] troubleBytes = new byte[0];
        try {
            troubleBytes = troubleString.getBytes("cp1251");
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
        }
        String parsingString = new String(troubleBytes, Charset.forName("UTF-8"));
        //String parsingString = new String(troubleBytes, Charset.forName("cp1251"));
        System.out.println(parsingString);
    }
}
